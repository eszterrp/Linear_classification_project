{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f354add",
   "metadata": {},
   "source": [
    "# Helper functions used in the Linear Classification project \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45077b7d",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5eafce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummies\n",
    "def get_dummies(df,columns):\n",
    "    import pandas as pd\n",
    "    return pd.get_dummies(data=df, columns=columns, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a9cd70",
   "metadata": {},
   "source": [
    "## Reweigthing predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227cc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the correction factor: \n",
    "def reweight(pi,q1,r1):\n",
    "    r0 = 1-r1\n",
    "    q0 = 1-q1\n",
    "    tot = pi*(q1/r1)+(1-pi)*(q0/r0)\n",
    "    w = pi*(q1/r1)\n",
    "    w /= tot\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f50ce1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweight_multi(pi,q,r=1/7):\n",
    "    w = []\n",
    "    q_r = [x / r for x in q]\n",
    "    for n in range(0, len(pi+1)):\n",
    "        tot = pi.loc[n]*pd.Series(q_r)\n",
    "        tot_s = sum(tot)\n",
    "        b = [x / tot_s for x in tot]\n",
    "        w.append(b)\n",
    "    w = np.array(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75084e82",
   "metadata": {},
   "source": [
    "## Resampling and evaluating their effect on the classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c4136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_and_evaluate(X,y,sampling, sampling_strat): \n",
    "    RANDOM_STATE = 42\n",
    "    score=list()\n",
    "    \n",
    "    from numpy import mean\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    if sampling ==\"under\":  \n",
    "        \n",
    "         # define undersampling strategy\n",
    "        undersample = RandomUnderSampler(sampling_strategy=sampling_strat, random_state=RANDOM_STATE)\n",
    "        X_under, y_under = undersample.fit_resample(X, y)\n",
    "        \n",
    "        # summarize class distribution\n",
    "        #print(Counter(y_under))\n",
    "        lregr = LogisticRegression(penalty='l2', C=100.0, \n",
    "                           fit_intercept=True, \n",
    "                           intercept_scaling=1, \n",
    "                           solver='liblinear', max_iter=500, random_state=RANDOM_STATE)\n",
    "\n",
    "        # fit model\n",
    "        lregr.fit(X_under, y_under)\n",
    "        \n",
    "        # make prediction\n",
    "        y_hat_under=lregr.predict_proba(X_under)\n",
    "\n",
    "        \n",
    "        # evaluate model\n",
    "        return roc_auc_score(y_true=y_under, y_score=y_hat_under[:,1])\n",
    "        #print('AUC Score of UnderSampling with', str(sampling_strat), \n",
    "        #      'sampling strategy: ', roc_auc_score(y_true=y_under, y_score=y_hat_under[:,1]))\n",
    "        \n",
    "    elif sampling ==\"tomek\": \n",
    "        \n",
    "        # define undersampling strategy\n",
    "        from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "        tomek = TomekLinks( sampling_strategy=sampling_strat) #random_state= 42\n",
    "        # unexpected error: TypeError: __init__() got an unexpected keyword argument 'random_state\n",
    "        # so that is why the random_state argument is not used in this case\n",
    "        \n",
    "        X_t, y_t = tomek.fit_resample(X, y)\n",
    "        \n",
    "        # summarize class distribution\n",
    "        #print(Counter(y_t))\n",
    "        lregr = LogisticRegression(penalty='l2', C=100.0, \n",
    "                           fit_intercept=True, \n",
    "                           intercept_scaling=1, \n",
    "                           solver='liblinear', max_iter=500, random_state=RANDOM_STATE)\n",
    "\n",
    "        # fit model\n",
    "        lregr.fit(X_t, y_t)\n",
    "        \n",
    "        # make prediction\n",
    "        y_hat_tomek=lregr.predict_proba(X_t)\n",
    "\n",
    "        \n",
    "        # evaluate model\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        return roc_auc_score(y_true=y_t, y_score=y_hat_tomek[:,1])\n",
    "        #print('AUC Score of UnderSampling using Tomek-links with', str(sampling_strat), \n",
    "         #     'sampling strategy: ', roc_auc_score(y_true=y_t, y_score=y_hat_tomek[:,1]))\n",
    "        \n",
    "    elif sampling==\"over\":\n",
    "        \n",
    "        # define oversampling strategy\n",
    "        oversample = RandomOverSampler(sampling_strategy=sampling_strat, random_state=RANDOM_STATE)\n",
    "        X_over, y_over = oversample.fit_resample(X, y)\n",
    "        \n",
    "        # summarize class distribution\n",
    "        #print(Counter(y_over))\n",
    "        lregr = LogisticRegression(penalty='l2', C=100.0, \n",
    "                           fit_intercept=True, \n",
    "                           intercept_scaling=1, \n",
    "                           solver='liblinear', max_iter=500, random_state=RANDOM_STATE)\n",
    "\n",
    "        # fit model\n",
    "        lregr.fit(X_over, y_over)\n",
    "        \n",
    "        # make prediction\n",
    "        y_hat_over=lregr.predict_proba(X_over)\n",
    "\n",
    "        \n",
    "        # evaluate model\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        return roc_auc_score(y_true=y_over, y_score=y_hat_over[:,1])\n",
    "        #print('AUC Score of OverSampling with', str(sampling_strat), \n",
    "        #      'sampling strategy: ', roc_auc_score(y_true=y_over, y_score=y_hat_over[:,1]))\n",
    "        \n",
    "    elif sampling==\"smote\":\n",
    "        \n",
    "        # define oversampling strategy\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "\n",
    "        smote = SMOTE(sampling_strategy=sampling_strat, random_state=RANDOM_STATE)\n",
    "        X_sm, y_sm = smote.fit_resample(X, y)\n",
    "        \n",
    "        # summarize class distribution\n",
    "        #print(Counter(y_sm))\n",
    "        lregr = LogisticRegression(penalty='l2', C=100.0, \n",
    "                           fit_intercept=True, \n",
    "                           intercept_scaling=1, \n",
    "                           solver='liblinear', max_iter=500, random_state=RANDOM_STATE)\n",
    "\n",
    "        # fit model\n",
    "        lregr.fit(X_sm, y_sm)\n",
    "        \n",
    "        # make prediction\n",
    "        y_hat_sm=lregr.predict_proba(X_sm)\n",
    "\n",
    "        \n",
    "        # evaluate model\n",
    "        from sklearn.metrics import roc_auc_score        \n",
    "        return roc_auc_score(y_true=y_sm, y_score=y_hat_sm[:,1])\n",
    "        #print('AUC Score of OverSampling using SMOTE with', str(sampling_strat), \n",
    "        #      'sampling strategy: ', roc_auc_score(y_true=y_sm, y_score=y_hat_sm[:,1]))\n",
    "    elif sampling==\"smotetomek\": \n",
    "        \n",
    "        from imblearn.combine import SMOTETomek \n",
    "        \n",
    "        smotetomek = SMOTETomek(sampling_strategy=sampling_strat, random_state=RANDOM_STATE)\n",
    "        X_smtl, y_smtl = smotetomek.fit_resample(X, y)\n",
    "\n",
    "        # summarize class distribution\n",
    "        #print(Counter(y_sm))\n",
    "        lregr = LogisticRegression(penalty='l2', C=100.0, \n",
    "                           fit_intercept=True, \n",
    "                           intercept_scaling=1, \n",
    "                           solver='liblinear', max_iter=500, random_state=RANDOM_STATE)\n",
    "\n",
    "        # fit model\n",
    "        lregr.fit(X_smtl, y_smtl)\n",
    "\n",
    "        # make prediction\n",
    "        y_hat_smtl=lregr.predict_proba(X_smtl)\n",
    "\n",
    "\n",
    "        # evaluate model\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        return roc_auc_score(y_true=y_smtl, y_score=y_hat_smtl[:,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90218916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluate_sampling_ratios(ratios,ratios_reversed, result_dictionary ):\n",
    "\n",
    "    for r1 in ratios:\n",
    "        for r2 in ratios_reversed: \n",
    "            try:\n",
    "\n",
    "                # define oversampling strategy\n",
    "                over = RandomOverSampler(sampling_strategy=r1)\n",
    "                # fit and apply the transform\n",
    "                X_over, y_over = over.fit_resample(X_scaled, y_original)\n",
    "\n",
    "                # define undersampling strategy\n",
    "                under = RandomUnderSampler(sampling_strategy=r2)\n",
    "                # fit and apply the transform\n",
    "                X_over_under, y_over_under = under.fit_resample(X_over, y_over)\n",
    "\n",
    "                # use a Logistic Regression setting based on previous finetuning results\n",
    "                # (high C value)\n",
    "                lregr = LogisticRegression(penalty='l2', C=1000.0, \n",
    "                                           fit_intercept=True, \n",
    "                                           intercept_scaling=1, \n",
    "                                           solver='liblinear', max_iter=500)\n",
    "\n",
    "                # fit model\n",
    "                lregr.fit(X_over_under, y_over_under)\n",
    "\n",
    "                # make prediction\n",
    "                y_hat_overunder=lregr.predict_proba(X_over_under)\n",
    "\n",
    "\n",
    "                # evaluate model\n",
    "                dic_key=str(\"OverSampler ratio: \" + str(r1) + \", UnderSampler ratio: \" + str(r2))\n",
    "                result_dictionary[dic_key]=roc_auc_score(y_true=y_over_under, y_score=y_hat_overunder[:,1])\n",
    "\n",
    "                # store ratios of best sampling strategy with the highest AUC score\n",
    "                sorted(result_dictionary.items(), key=lambda item: item[1], reverse=True)[:1]\n",
    "                l=list(dict(sorted(result_dictionary.items(), key=lambda item: item[1], reverse=True)[:1]).keys())\n",
    "                ratios=list(flatten([re.findall(r\"0\\.\\d{1}\", x) for x in l]))\n",
    "                overs_r=float(ratios[0])\n",
    "                unders_r=float(ratios[1])\n",
    "                return overs_r, unders_r\n",
    "            except ValueError: \n",
    "                pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3da3bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_best_sampling(overs_r, unders_r, model):\n",
    "    \n",
    "    \n",
    "    \n",
    "    over = RandomOverSampler(sampling_strategy=float(overs_r))\n",
    "    # fit and apply the transform\n",
    "    X_over, y_over = over.fit_resample(X_scaled, y_original)\n",
    "\n",
    "    # define undersampling strategy\n",
    "    under = RandomUnderSampler(sampling_strategy=float(unders_r))\n",
    "    # fit and apply the transform\n",
    "    X_over_under, y_over_under = under.fit_resample(X_over, y_over)\n",
    "\n",
    "    model = LogisticRegression(penalty='l2', C=1000.0,  \n",
    "                               solver='liblinear', max_iter=500)\n",
    "\n",
    "\n",
    "    model.fit(X_over_under, y_over_under)\n",
    "    y_hat=model.predict_proba(X_over_under)\n",
    "\n",
    "    from sklearn import metrics\n",
    "    get_auc(y_over_under,y_hat[:,1] , class_labels, column=1, plot=True) \n",
    "\n",
    "    # Classification report\n",
    "    print(\"Classification report of the model: \\n\", \n",
    "          metrics.classification_report(y_over_under, model.predict(X_over_under)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a8a538",
   "metadata": {},
   "source": [
    "## Model finetuning (finding the best hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11381944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model(X,y, model, class_weight, solvers, penalty, c_values):\n",
    "    # define grid search\n",
    "    grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='roc_auc',error_score=0)\n",
    "    grid_result = grid_search.fit(X, y)\n",
    "\n",
    "    # summarize results\n",
    "    \n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    return (grid_result.best_score_, grid_result.best_params_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef0131",
   "metadata": {},
   "source": [
    "## Predict and evaluate classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e17f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_evaluate_binary(X,y, model, crossval=\"Yes\"):\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import pandas as pd\n",
    "    \n",
    "    model.fit(X,y)\n",
    "    y_hat=model.predict_proba(X)\n",
    "    \n",
    "    # reweighting \n",
    "    q1 = y.sum()/len(y)\n",
    "    r1 = 0.5\n",
    "    y_hat_corr=reweight(y_hat[:,1], q1,r1)\n",
    "    \n",
    "    \n",
    "    ### Evaluate Model ###\n",
    "    y_pred_new = [1 if pi >= 0.25 else 0 for pi in y_hat_corr]\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(\"Confusion Matrix \\n\")\n",
    "    # insample_labels = model.predict(X)\n",
    "    cm =  confusion_matrix(y_pred=y_pred_new, y_true=y, labels=[0,1])\n",
    "    print (cm)\n",
    "    \n",
    "    # Plotting confusion matrix (custom help function)\n",
    "    df_cm = pd.DataFrame(cm, index = [i for i in class_labels],\n",
    "                  columns = [i for i in class_labels])\n",
    "    sns.set(font_scale=1)\n",
    "    sns.heatmap(df_cm, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"Real label\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC AUC score\n",
    "    #get_auc(y_original,y_hat_corr , class_labels, column=1, plot=True) \n",
    "    fpr, tpr, _ = roc_curve(y == 1, y_hat_corr,drop_intermediate = False)\n",
    "    roc_auc = roc_auc_score(y_true=y, y_score=y_hat_corr)\n",
    "    print (\"AUC: \", roc_auc)\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    # Classification report\n",
    "    print(\"Classification report of the model: \\n\", \n",
    "      metrics.classification_report(y,y_pred_new))\n",
    "    \n",
    "    if crossval==\"Yes\":\n",
    "        # cross validation\n",
    "        print(\"Confusion matrix of in-sample cross-validation:\")\n",
    "\n",
    "        from sklearn.model_selection import cross_val_predict as cvp\n",
    "        y_hat_cv = cvp(model, X, y, cv=100)\n",
    "\n",
    "        cm2 =  confusion_matrix(y_pred=y_hat_cv, y_true=y, labels=[0,1])\n",
    "        #print(cm2)\n",
    "        df_cm2 = pd.DataFrame(cm2, index = [i for i in class_labels],\n",
    "                      columns = [i for i in class_labels])\n",
    "        sns.set(font_scale=1)\n",
    "        sns.heatmap(df_cm, annot=True, fmt='g', cmap='Blues')\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "        plt.ylabel(\"Real label\")\n",
    "        plt.show()\n",
    "        \n",
    "    elif crossval==\"No\":\n",
    "        print(\"No cross-validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124517de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_evaluate_multiclass(X,y,model):\n",
    "    \n",
    "    model.fit(X, y)\n",
    "\n",
    "    # define the evaluation procedure with cross-validation accuracy scores\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "    # evaluate the model\n",
    "    score = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "    # mean accuracy \n",
    "    print(\"Mean accuracy result of in-sample cross-validation:\", np.mean(score))\n",
    "    \n",
    "    y_hat=model.predict(X)\n",
    "    class_report=metrics.classification_report(y,y_hat )\n",
    "    print(\"\\n\")\n",
    "    print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e2fadc",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfc04a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_wo_crossval(X,y, y_corr, model, class_labels):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import pandas as pd\n",
    "    \n",
    "    #y_hat=model.predict(X)\n",
    "    cm =  confusion_matrix(y_pred=y_corr, y_true=y, labels=class_labels)\n",
    "    #print(cm)\n",
    "    df_cm = pd.DataFrame(cm, index = [i for i in class_labels],\n",
    "                  columns = [i for i in class_labels])\n",
    "    sns.set(font_scale=1)\n",
    "    sns.heatmap(df_cm, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"Real label\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d96f9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_(X,y, y_corr, model, class_labels):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import pandas as pd\n",
    "    \n",
    "    #y_hat=model.predict(X)\n",
    "    cm =  confusion_matrix(y_pred=y_corr, y_true=y, labels=class_labels)\n",
    "    #print(cm)\n",
    "    df_cm = pd.DataFrame(cm, index = [i for i in class_labels],\n",
    "                  columns = [i for i in class_labels])\n",
    "    sns.set(font_scale=1)\n",
    "    sns.heatmap(df_cm, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"Real label\")\n",
    "    plt.show()\n",
    "    \n",
    "# use in-sample cross-validation\n",
    "    print(\"Confusion matrix of in-sample cross-validation:\")\n",
    "    \n",
    "    from sklearn.model_selection import cross_val_predict as cvp\n",
    "    y_hat_cv = cvp(model, X, y, cv=100)\n",
    "    \n",
    "    cm2 =  confusion_matrix(y_pred=y_hat_cv, y_true=y, labels=class_labels)\n",
    "    #print(cm2)\n",
    "    df_cm2 = pd.DataFrame(cm2, index = [i for i in class_labels],\n",
    "                  columns = [i for i in class_labels])\n",
    "    sns.set(font_scale=1)\n",
    "    sns.heatmap(df_cm, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"Real label\")\n",
    "    plt.show()\n",
    "#\n",
    "    \n",
    "    \n",
    "    #print (cm2)\n",
    "    ## Plotting confusion matrix (custom help function)\n",
    "#\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22cd42d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the Euler number to the power of its coefficient to find the importance.\n",
    "def feature_importance_plot(model):\n",
    "    feature_importance = pd.DataFrame(feature_names, columns = [\"feature\"])\n",
    "    feature_importance[\"importance\"] = model.coef_[0]\n",
    "    feature_importance[\"importance_abs_value\"] = feature_importance[\"importance\"].abs()\n",
    "\n",
    "    feature_importance_top10 = feature_importance.sort_values(by = [\"importance_abs_value\"], ascending=True).head(15)\n",
    "\n",
    "    fig = plt.figure(figsize = (20,25))\n",
    "    ax = feature_importance_top10.plot.barh(x='feature', y='importance', \n",
    "                                               title=\"Top 15 most important variables according to their LogReg coefficients \")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac7227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "def get_auc(y, y_pred_probabilities, class_labels, column =1, plot = True):\n",
    "    \"\"\"Plots ROC AUC\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y == column, y_pred_probabilities,drop_intermediate = False)\n",
    "    roc_auc = roc_auc_score(y_true=y, y_score=y_pred_probabilities)\n",
    "    print (\"AUC: \", roc_auc)\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
